{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit: 'Front Page of the Internet'\n",
    "\n",
    "![](./images/reddit_search_result.png)\n",
    "\n",
    "## What does it take for a post to get there?\n",
    "![](./images/reddit_top.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Query June 2017 Data for 'default' subreddits\n",
    "\n",
    "![](./images/bg_table_desc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Going with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# my local spark install\n",
    "findspark.init('/Users/dreyco676/spark-2.2.0-bin-hadoop2.7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# create spark contexts\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can import a CSV to a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv('data/reddit_defaults_june17.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087311"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, created_utc: string, subreddit: string, author: string, domain: string, url: string, num_comments: string, score: string, ups: string, downs: string, title: string, selftext: string, saved: string, id: string, from_kind: string, gilded: string, from: string, stickied: string, retrieved_on: string, over_18: string, thumbnail: string, subreddit_id: string, hide_score: string, link_flair_css_class: string, author_flair_css_class: string, archived: string, is_self: string, from_id: string, permalink: string, name: string, author_flair_text: string, quarantine: string, link_flair_text: string, distinguished: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ಠ_ಠ\n",
    "### Even though we told it to infer the schema it still set everything to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_cols = ['num_comments', 'score', 'ups', 'downs', 'over_18']\n",
    "bool_cols = ['saved', 'gilded', 'stickied', 'hide_score', 'archived', 'is_self', 'quarantine']\n",
    "date_cols = ['created_utc', 'retrieved_on']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|created_utc|\n",
      "+-----------+\n",
      "| 1497045651|\n",
      "| 1498078666|\n",
      "| 1498318066|\n",
      "| 1497534495|\n",
      "| 1498002534|\n",
      "| 1498821879|\n",
      "| 1498490141|\n",
      "| 1497556307|\n",
      "| 1498388671|\n",
      "| 1498143572|\n",
      "| 1498444669|\n",
      "| 1497528723|\n",
      "| 1496679510|\n",
      "| 1497188637|\n",
      "| 1497751261|\n",
      "| 1496930590|\n",
      "| 1497231917|\n",
      "| 1498746229|\n",
      "| 1496829959|\n",
      "| 1497894195|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['created_utc']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dates are in Unix Epoch time \n",
    "(seconds since 1970-01-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "for col in date_cols:\n",
    "    df = df.withColumn(col, from_unixtime(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        created_utc|\n",
      "+-------------------+\n",
      "|2017-06-09 17:00:51|\n",
      "|2017-06-21 15:57:46|\n",
      "|2017-06-24 10:27:46|\n",
      "|2017-06-15 08:48:15|\n",
      "|2017-06-20 18:48:54|\n",
      "|2017-06-30 06:24:39|\n",
      "|2017-06-26 10:15:41|\n",
      "|2017-06-15 14:51:47|\n",
      "|2017-06-25 06:04:31|\n",
      "|2017-06-22 09:59:32|\n",
      "|2017-06-25 21:37:49|\n",
      "|2017-06-15 07:12:03|\n",
      "|2017-06-05 11:18:30|\n",
      "|2017-06-11 08:43:57|\n",
      "|2017-06-17 21:01:01|\n",
      "|2017-06-08 09:03:10|\n",
      "|2017-06-11 20:45:17|\n",
      "|2017-06-29 09:23:49|\n",
      "|2017-06-07 05:05:59|\n",
      "|2017-06-19 12:43:15|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['created_utc']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use langid module to classify the language to make sure we are applying the correct cleanup actions for English\n",
    "# https://github.com/saffsd/langid.py\n",
    "def check_lang(data_str):\n",
    "    predict_lang = langid.classify(data_str)\n",
    "    if predict_lang[1] >= .9:\n",
    "        language = predict_lang[0]\n",
    "    else:\n",
    "        language = 'NA'\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Register all the functions in Preproc with Spark Context\n",
    "check_lang_udf = udf(pp.check_lang, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict language and filter out those with less than 90% chance of being English\n",
    "lang_df = data_df.withColumn(\"lang\", check_lang_udf(data_df[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/reddit/reddit/wiki/JSON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
