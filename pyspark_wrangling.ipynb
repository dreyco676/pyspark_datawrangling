{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit: 'Front Page of the Internet'\n",
    "\n",
    "![](./images/reddit_search_result.png)\n",
    "\n",
    "## What does it take for a post to get there?\n",
    "![](./images/reddit_top.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Query June 2017 Data for 'default' subreddits\n",
    "\n",
    "![](./images/bg_table_desc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Going with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "# my local spark install\n",
    "findspark.init('/Users/dreyco676/spark-2.2.0-bin-hadoop2.7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# create spark contexts\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can import a CSV to a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv('data/reddit_defaults_june17.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1087311"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Know your data\n",
    "Data Science can't be done in a vacuum. You need to know what your data represents.\n",
    "\n",
    "https://github.com/reddit/reddit/wiki/JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple fields are not needed.\n",
    "* `saved` is only applicable to the person that pulled the data\n",
    "\n",
    "* `id` is a duplicate of the `author` field\n",
    "\n",
    "* `thumbnail` is a randomly assigned reddit link for posts that have a link\n",
    "\n",
    "* `subreddit_id` is a duplicate of `subreddit` field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['saved', 'id', 'thumbnail', 'subreddit_id']\n",
    "for col in drop_cols:\n",
    "    df.drop(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some records are not valid.\n",
    "Sometimes users delete their posts, filter them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.filter(df[\"title\"] != \"[deleted]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "945380"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_utc', 'string'),\n",
       " ('subreddit', 'string'),\n",
       " ('author', 'string'),\n",
       " ('domain', 'string'),\n",
       " ('url', 'string'),\n",
       " ('num_comments', 'string'),\n",
       " ('score', 'string'),\n",
       " ('ups', 'string'),\n",
       " ('downs', 'string'),\n",
       " ('title', 'string'),\n",
       " ('selftext', 'string'),\n",
       " ('saved', 'string'),\n",
       " ('id', 'string'),\n",
       " ('from_kind', 'string'),\n",
       " ('gilded', 'string'),\n",
       " ('from', 'string'),\n",
       " ('stickied', 'string'),\n",
       " ('retrieved_on', 'string'),\n",
       " ('over_18', 'string'),\n",
       " ('thumbnail', 'string'),\n",
       " ('subreddit_id', 'string'),\n",
       " ('hide_score', 'string'),\n",
       " ('link_flair_css_class', 'string'),\n",
       " ('author_flair_css_class', 'string'),\n",
       " ('archived', 'string'),\n",
       " ('is_self', 'string'),\n",
       " ('from_id', 'string'),\n",
       " ('permalink', 'string'),\n",
       " ('name', 'string'),\n",
       " ('author_flair_text', 'string'),\n",
       " ('quarantine', 'string'),\n",
       " ('link_flair_text', 'string'),\n",
       " ('distinguished', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ಠ_ಠ\n",
    "### Even though we told it to infer the schema it still set everything to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_cols = ['num_comments', 'score', 'ups', 'downs', 'over_18']\n",
    "bool_cols = ['gilded', 'stickied', 'hide_score', 'archived', 'is_self', 'quarantine']\n",
    "date_cols = ['created_utc', 'retrieved_on']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|created_utc|\n",
      "+-----------+\n",
      "| 1497045651|\n",
      "| 1498078666|\n",
      "| 1498318066|\n",
      "| 1497534495|\n",
      "| 1498002534|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['created_utc']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dates are in Unix Epoch time \n",
    "(seconds since 1970-01-01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "for col in date_cols:\n",
    "    df = df.withColumn(col, from_unixtime(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        created_utc|\n",
      "+-------------------+\n",
      "|2017-06-09 17:00:51|\n",
      "|2017-06-21 15:57:46|\n",
      "|2017-06-24 10:27:46|\n",
      "|2017-06-15 08:48:15|\n",
      "|2017-06-20 18:48:54|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['created_utc']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Strings to Integers\n",
    "(never cast things you wouldn't do math on, ie IDs, phone numbers, zip codes, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "for col in int_cols:\n",
    "    df = df.withColumn(col, df[col].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Strings to Booleans\n",
    "(Yes/No, 1/0, True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "for col in bool_cols:\n",
    "    df = df.withColumn(col, df[col].cast(BooleanType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_utc', 'string'),\n",
       " ('subreddit', 'string'),\n",
       " ('author', 'string'),\n",
       " ('domain', 'string'),\n",
       " ('url', 'string'),\n",
       " ('num_comments', 'int'),\n",
       " ('score', 'int'),\n",
       " ('ups', 'int'),\n",
       " ('downs', 'int'),\n",
       " ('title', 'string'),\n",
       " ('selftext', 'string'),\n",
       " ('saved', 'string'),\n",
       " ('id', 'string'),\n",
       " ('from_kind', 'string'),\n",
       " ('gilded', 'boolean'),\n",
       " ('from', 'string'),\n",
       " ('stickied', 'boolean'),\n",
       " ('retrieved_on', 'string'),\n",
       " ('over_18', 'int'),\n",
       " ('thumbnail', 'string'),\n",
       " ('subreddit_id', 'string'),\n",
       " ('hide_score', 'boolean'),\n",
       " ('link_flair_css_class', 'string'),\n",
       " ('author_flair_css_class', 'string'),\n",
       " ('archived', 'boolean'),\n",
       " ('is_self', 'boolean'),\n",
       " ('from_id', 'string'),\n",
       " ('permalink', 'string'),\n",
       " ('name', 'string'),\n",
       " ('author_flair_text', 'string'),\n",
       " ('quarantine', 'boolean'),\n",
       " ('link_flair_text', 'string'),\n",
       " ('distinguished', 'string')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use langid module to classify the language to make sure we are applying the correct cleanup actions for English\n",
    "# https://github.com/saffsd/langid.py\n",
    "import langid\n",
    "\n",
    "def check_lang(data_str):\n",
    "    language = 'NA'\n",
    "    if data_str is not None:\n",
    "        predict_lang = langid.classify(data_str)\n",
    "        if predict_lang[1] >= .7:\n",
    "            language = predict_lang[0]\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Register all the functions in Preproc with Spark Context\n",
    "check_lang_udf = udf(check_lang, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict language\n",
    "df = df.withColumn(\"language\", check_lang_udf(df[\"selftext\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            selftext|language|\n",
      "+--------------------+--------+\n",
      "|                null|      NA|\n",
      "|              Marker|      en|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|           [deleted]|      en|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|                null|      NA|\n",
      "|           [deleted]|      en|\n",
      "|                null|      NA|\n",
      "| illustration art...|      NA|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['selftext', 'language']].show(20, truncate=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created_utc --day of week, hour of day\n",
    "subreddit\n",
    "author\n",
    "domain\n",
    "url\n",
    "num_comments\n",
    "score\n",
    "ups\n",
    "downs\n",
    "title\n",
    "selftext\n",
    "saved --data collection artifact\n",
    "id --duplicate of author\n",
    "from_kind\n",
    "gilded\n",
    "from\n",
    "stickied\n",
    "retrieved_on\n",
    "over_18\n",
    "thumbnail --info contained in is_self\n",
    "subreddit_id --duplicate of subreddit\n",
    "hide_score\n",
    "link_flair_css_class\n",
    "author_flair_css_class\n",
    "archived\n",
    "is_self\n",
    "from_id\n",
    "permalink\n",
    "name\n",
    "author_flair_text\n",
    "quarantine\n",
    "link_flair_text\n",
    "distinguished"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
